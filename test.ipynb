{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the neural network class with hard-coded layer sizes\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        # Define layers with hard-coded sizes\n",
    "        self.fc1 = nn.Linear(1, 64)  # Input layer: 1 feature to 64 neurons\n",
    "        self.relu = nn.ReLU()         # Activation function\n",
    "        self.fc2 = nn.Linear(64, 64)  # Hidden layer: 64 neurons to 64 neurons\n",
    "        self.fc3 = nn.Linear(64, 1)   # Output layer: 64 neurons to 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = RegressionNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(name):\n",
    "    x = torch.FloatTensor([1])\n",
    "    with torch.no_grad():\n",
    "        traced_cell = torch.jit.trace(model, (x))\n",
    "    torch.jit.save(traced_cell, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('untrained.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 40446417.3750\n",
      "Epoch [20/1000], Loss: 34416552.2500\n",
      "Epoch [30/1000], Loss: 26453231.5000\n",
      "Epoch [40/1000], Loss: 19114477.8438\n",
      "Epoch [50/1000], Loss: 14117737.7500\n",
      "Epoch [60/1000], Loss: 10769047.2812\n",
      "Epoch [70/1000], Loss: 8210578.4375\n",
      "Epoch [80/1000], Loss: 6122325.1641\n",
      "Epoch [90/1000], Loss: 4507814.5703\n",
      "Epoch [100/1000], Loss: 3320112.9297\n",
      "Epoch [110/1000], Loss: 2479151.0586\n",
      "Epoch [120/1000], Loss: 1915205.0762\n",
      "Epoch [130/1000], Loss: 1510723.1289\n",
      "Epoch [140/1000], Loss: 1220924.4902\n",
      "Epoch [150/1000], Loss: 986190.5410\n",
      "Epoch [160/1000], Loss: 771167.3809\n",
      "Epoch [170/1000], Loss: 577398.6431\n",
      "Epoch [180/1000], Loss: 427197.4917\n",
      "Epoch [190/1000], Loss: 320023.1143\n",
      "Epoch [200/1000], Loss: 242259.6755\n",
      "Epoch [210/1000], Loss: 179738.0940\n",
      "Epoch [220/1000], Loss: 135140.6572\n",
      "Epoch [230/1000], Loss: 106018.5110\n",
      "Epoch [240/1000], Loss: 86778.7143\n",
      "Epoch [250/1000], Loss: 72656.6942\n",
      "Epoch [260/1000], Loss: 56905.2744\n",
      "Epoch [270/1000], Loss: 45035.9624\n",
      "Epoch [280/1000], Loss: 39643.4561\n",
      "Epoch [290/1000], Loss: 37907.6698\n",
      "Epoch [300/1000], Loss: 35269.0618\n",
      "Epoch [310/1000], Loss: 31635.4416\n",
      "Epoch [320/1000], Loss: 29075.7755\n",
      "Epoch [330/1000], Loss: 26699.5127\n",
      "Epoch [340/1000], Loss: 25295.0248\n",
      "Epoch [350/1000], Loss: 24060.9561\n",
      "Epoch [360/1000], Loss: 22566.8802\n",
      "Epoch [370/1000], Loss: 21774.6771\n",
      "Epoch [380/1000], Loss: 19616.4442\n",
      "Epoch [390/1000], Loss: 15199.1765\n",
      "Epoch [400/1000], Loss: 9468.0398\n",
      "Epoch [410/1000], Loss: 9007.8292\n",
      "Epoch [420/1000], Loss: 8061.7123\n",
      "Epoch [430/1000], Loss: 7200.1838\n",
      "Epoch [440/1000], Loss: 10634.0227\n",
      "Epoch [450/1000], Loss: 9526.9570\n",
      "Epoch [460/1000], Loss: 8484.2404\n",
      "Epoch [470/1000], Loss: 8962.5317\n",
      "Epoch [480/1000], Loss: 8332.5063\n",
      "Epoch [490/1000], Loss: 8606.3019\n",
      "Epoch [500/1000], Loss: 8104.2370\n",
      "Epoch [510/1000], Loss: 8343.3029\n",
      "Epoch [520/1000], Loss: 7755.8964\n",
      "Epoch [530/1000], Loss: 7721.1076\n",
      "Epoch [540/1000], Loss: 7395.6509\n",
      "Epoch [550/1000], Loss: 7306.9927\n",
      "Epoch [560/1000], Loss: 7253.8678\n",
      "Epoch [570/1000], Loss: 7195.3869\n",
      "Epoch [580/1000], Loss: 7089.5006\n",
      "Epoch [590/1000], Loss: 7124.7438\n",
      "Epoch [600/1000], Loss: 6991.9709\n",
      "Epoch [610/1000], Loss: 6781.3432\n",
      "Epoch [620/1000], Loss: 6427.3011\n",
      "Epoch [630/1000], Loss: 6197.1679\n",
      "Epoch [640/1000], Loss: 4728.8790\n",
      "Epoch [650/1000], Loss: 3319.3114\n",
      "Epoch [660/1000], Loss: 3905.8903\n",
      "Epoch [670/1000], Loss: 4050.6029\n",
      "Epoch [680/1000], Loss: 4170.4618\n",
      "Epoch [690/1000], Loss: 4185.2734\n",
      "Epoch [700/1000], Loss: 4138.9628\n",
      "Epoch [710/1000], Loss: 4096.0316\n",
      "Epoch [720/1000], Loss: 3900.6239\n",
      "Epoch [730/1000], Loss: 3840.3397\n",
      "Epoch [740/1000], Loss: 3768.1147\n",
      "Epoch [750/1000], Loss: 3722.1410\n",
      "Epoch [760/1000], Loss: 3694.9364\n",
      "Epoch [770/1000], Loss: 3666.0352\n",
      "Epoch [780/1000], Loss: 3627.8786\n",
      "Epoch [790/1000], Loss: 3601.3344\n",
      "Epoch [800/1000], Loss: 3569.3007\n",
      "Epoch [810/1000], Loss: 3534.5762\n",
      "Epoch [820/1000], Loss: 3517.8482\n",
      "Epoch [830/1000], Loss: 3495.4517\n",
      "Epoch [840/1000], Loss: 3475.9680\n",
      "Epoch [850/1000], Loss: 3461.8025\n",
      "Epoch [860/1000], Loss: 3436.3023\n",
      "Epoch [870/1000], Loss: 3435.8654\n",
      "Epoch [880/1000], Loss: 3424.2503\n",
      "Epoch [890/1000], Loss: 3403.8607\n",
      "Epoch [900/1000], Loss: 3383.5977\n",
      "Epoch [910/1000], Loss: 3393.4679\n",
      "Epoch [920/1000], Loss: 3373.0385\n",
      "Epoch [930/1000], Loss: 3355.3546\n",
      "Epoch [940/1000], Loss: 3344.9086\n",
      "Epoch [950/1000], Loss: 3330.0176\n",
      "Epoch [960/1000], Loss: 3319.3245\n",
      "Epoch [970/1000], Loss: 3306.5589\n",
      "Epoch [980/1000], Loss: 3293.3217\n",
      "Epoch [990/1000], Loss: 3284.2864\n",
      "Epoch [1000/1000], Loss: 3273.5698\n"
     ]
    }
   ],
   "source": [
    "from load_dataloader import *\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "\n",
    "# Example input data\n",
    "dataloader = load_dataloader(1)\n",
    "\n",
    "# Training loop (for demonstration purposes)\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)  # Compute loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:  # Print loss every 10 epochs\n",
    "        print(f'Epoch [{epoch+1}/1000], Loss: {running_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
